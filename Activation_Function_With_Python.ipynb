{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "import numpy as np"
      ],
      "metadata": {
        "id": "h7RgQsL8K76m"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Linear Activation Funtion**"
      ],
      "metadata": {
        "id": "1deR5hMOKJQ2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def LinearAF(x):\n",
        "  return x"
      ],
      "metadata": {
        "id": "cwdopg08KHBu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**RELU**"
      ],
      "metadata": {
        "id": "6YOJDkTSLGjW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def relu(x):\n",
        "  return np.maximum(0,x)"
      ],
      "metadata": {
        "id": "Y7T6yCcGKTdm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Leaky RELU**"
      ],
      "metadata": {
        "id": "A_JmiI2iL4Qe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def leaky_relu(x, alpha=0.01):\n",
        "    return np.where(x > 0, x, alpha * x)"
      ],
      "metadata": {
        "id": "dUPRxw8RL3de"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Exponential Relu**"
      ],
      "metadata": {
        "id": "L79jXIcDXWup"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def ELU(x, alpha=0.01):\n",
        "    return np.where(x > 0, x, alpha * (np.exp(x) - 1))"
      ],
      "metadata": {
        "id": "-qX9EiYGXWSK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Sigmoid**"
      ],
      "metadata": {
        "id": "rHHmldHHLef2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def SigmoidAF(x):\n",
        "  return (1/(1+np.exp(-x)))"
      ],
      "metadata": {
        "id": "_rTygxYVLI9u"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "TanH Activation Funciton"
      ],
      "metadata": {
        "id": "Qdul-dy2Ln8F"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def tanh(x):\n",
        "    return np.tanh(x)"
      ],
      "metadata": {
        "id": "Ij02Nmq1LhXG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Soft Max**"
      ],
      "metadata": {
        "id": "6cOcsPM0X6aJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def softmax(x):\n",
        "    exp_x = np.exp(x - np.max(x))  # Stability improvement by subtracting max(x)\n",
        "    return exp_x / np.sum(exp_x)\n"
      ],
      "metadata": {
        "id": "t43W07M-LdFE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Example usage\n",
        "x = np.array([-2, -1, 0, 1, 2])\n",
        "\n",
        "print(\"Linear:\", LinearAF(x))\n",
        "print(\"Sigmoid:\", SigmoidAF(x))\n",
        "print(\"Tanh:\", tanh(x))\n",
        "print(\"ReLU:\", relu(x))\n",
        "print(\"Leaky ReLU:\", leaky_relu(x))\n",
        "print(\"ELU:\", ELU(x))\n",
        "print(\"Softmax:\", softmax(x))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_5zjxt4AYmc6",
        "outputId": "6e858599-0fd4-45dd-a15a-9367ebf9a148"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Linear: [-2 -1  0  1  2]\n",
            "Sigmoid: [0.11920292 0.26894142 0.5        0.73105858 0.88079708]\n",
            "Tanh: [-0.96402758 -0.76159416  0.          0.76159416  0.96402758]\n",
            "ReLU: [0 0 0 1 2]\n",
            "Leaky ReLU: [-0.02 -0.01  0.    1.    2.  ]\n",
            "ELU: [-0.00864665 -0.00632121  0.          1.          2.        ]\n",
            "Softmax: [0.01165623 0.03168492 0.08612854 0.23412166 0.63640865]\n"
          ]
        }
      ]
    }
  ]
}